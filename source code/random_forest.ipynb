{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"../processing_2/data_all.csv\", parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Campagne', 'Region', 'Site', 'Famille', 'Variete', 'Num Parcelle',\n",
       "       'CodeTracabilite', 'Date', 'Tonnage', 'is_train', 'Recolte',\n",
       "       'irrigation_cumulee_1', 'irrigation_cumulee_2', 'irrigation_cumulee_3',\n",
       "       'irrigation_cumulee_4', 'irrigation_cumulee_5', 'irrigation_cumulee_6',\n",
       "       'irrigation_cumulee_7', 'irrigation_cumulee_8', 'irrigation_cumulee_9',\n",
       "       'irrigation_cumulee_10', 'irrigation_cumulee_11',\n",
       "       'irrigation_cumulee_12', 'N_cumulee_1', 'P_cumulee_1', 'K_cumulee_1',\n",
       "       'N_cumulee_2', 'P_cumulee_2', 'K_cumulee_2', 'N_cumulee_3',\n",
       "       'P_cumulee_3', 'K_cumulee_3', 'N_cumulee_4', 'P_cumulee_4',\n",
       "       'K_cumulee_4', 'N_cumulee_5', 'P_cumulee_5', 'K_cumulee_5',\n",
       "       'N_cumulee_6', 'P_cumulee_6', 'K_cumulee_6', 'N_cumulee_7',\n",
       "       'P_cumulee_7', 'K_cumulee_7', 'N_cumulee_8', 'P_cumulee_8',\n",
       "       'K_cumulee_8', 'N_cumulee_9', 'P_cumulee_9', 'K_cumulee_9',\n",
       "       'N_cumulee_10', 'P_cumulee_10', 'K_cumulee_10', 'N_cumulee_11',\n",
       "       'P_cumulee_11', 'K_cumulee_11', 'N_cumulee_12', 'P_cumulee_12',\n",
       "       'K_cumulee_12', 'Id', 'Date Plantation', 'Date Arrachage',\n",
       "       'Porte Greffe', 'Sup Debut Camp', 'Sup Plantee', 'Sup Arrachee',\n",
       "       'Sup Fin Camp', 'Tree Age', 'Days to Arrachage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3943, 69)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonnage to float\n",
    "all_data['Tonnage'] = all_data['Tonnage'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Split the date\n",
    "all_data['Year'] = all_data['Date'].dt.year\n",
    "all_data['Month'] = all_data['Date'].dt.month\n",
    "all_data['Day'] = all_data['Date'].dt.day\n",
    "\n",
    "\n",
    "duplicates = all_data.duplicated(['CodeTracabilite', 'Num Parcelle', 'Campagne', 'Region', 'Site', 'Famille', 'Variete', 'Recolte'], keep=False)\n",
    "\n",
    "# Then, apply the condition\n",
    "condition = (all_data['Sup Debut Camp'] + all_data['Sup Plantee'] != all_data['Sup Arrachee'] + all_data['Sup Fin Camp'])\n",
    "\n",
    "# Combine both conditions\n",
    "to_drop = duplicates & condition\n",
    "\n",
    "# Drop the rows that satisfy both condgitions\n",
    "all_data = all_data[~to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month_index in range(12):\n",
    "    all_data[f'irrigation_cumulee_{month_index + 1}'] = all_data[f'irrigation_cumulee_{month_index + 1}'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3903, 72)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Campagne', 'Region', 'Site', 'Famille', 'Variete', 'Num Parcelle',\n",
       "       'CodeTracabilite', 'Date', 'Tonnage', 'is_train', 'Recolte',\n",
       "       'irrigation_cumulee_1', 'irrigation_cumulee_2', 'irrigation_cumulee_3',\n",
       "       'irrigation_cumulee_4', 'irrigation_cumulee_5', 'irrigation_cumulee_6',\n",
       "       'irrigation_cumulee_7', 'irrigation_cumulee_8', 'irrigation_cumulee_9',\n",
       "       'irrigation_cumulee_10', 'irrigation_cumulee_11',\n",
       "       'irrigation_cumulee_12', 'N_cumulee_1', 'P_cumulee_1', 'K_cumulee_1',\n",
       "       'N_cumulee_2', 'P_cumulee_2', 'K_cumulee_2', 'N_cumulee_3',\n",
       "       'P_cumulee_3', 'K_cumulee_3', 'N_cumulee_4', 'P_cumulee_4',\n",
       "       'K_cumulee_4', 'N_cumulee_5', 'P_cumulee_5', 'K_cumulee_5',\n",
       "       'N_cumulee_6', 'P_cumulee_6', 'K_cumulee_6', 'N_cumulee_7',\n",
       "       'P_cumulee_7', 'K_cumulee_7', 'N_cumulee_8', 'P_cumulee_8',\n",
       "       'K_cumulee_8', 'N_cumulee_9', 'P_cumulee_9', 'K_cumulee_9',\n",
       "       'N_cumulee_10', 'P_cumulee_10', 'K_cumulee_10', 'N_cumulee_11',\n",
       "       'P_cumulee_11', 'K_cumulee_11', 'N_cumulee_12', 'P_cumulee_12',\n",
       "       'K_cumulee_12', 'Id', 'Date Plantation', 'Date Arrachage',\n",
       "       'Porte Greffe', 'Sup Debut Camp', 'Sup Plantee', 'Sup Arrachee',\n",
       "       'Sup Fin Camp', 'Tree Age', 'Days to Arrachage', 'Year', 'Month',\n",
       "       'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['ndmi_missingness'] = all_data['ndmi'].apply(\n",
    "#     lambda x: 0 if x != 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['day'] = all_data['Date'].apply(lambda x: x.day)\n",
    "all_data['month'] = all_data['Date'].apply(lambda x: x.month)\n",
    "all_data['year'] = all_data['Date'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "cat_cols = ['Region', 'Site', 'Variete', 'Porte Greffe']\n",
    "cat_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "cols_to_impute_mean = ['Days to Arrachage', 'Tree Age'] + ['Sup Debut Camp']\n",
    "mean_imputer = SimpleImputer(strategy='mean', add_indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrigation_columns = [f'irrigation_cumulee_{i}' for i in range(1, 13)]\n",
    "\n",
    "# Calculate the median for these columns in each row\n",
    "row_mean = all_data[irrigation_columns].mean(axis=1)\n",
    "\n",
    "# Replace NaN values in each column with the row median\n",
    "for col in irrigation_columns:\n",
    "    all_data[col] = all_data[col].fillna(row_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns to impute with mean\n",
    "cols_to_impute_mean = ['Days to Arrachage', 'Tree Age', 'Sup Debut Camp'] + irrigation_columns\n",
    "\n",
    "# Initialize the mean imputer with an option to add indicators for imputed values\n",
    "mean_imputer = SimpleImputer(strategy='mean', add_indicator=True)\n",
    "\n",
    "# Apply the imputer to the specified columns\n",
    "imputed_data = mean_imputer.fit_transform(all_data[cols_to_impute_mean])\n",
    "\n",
    "# Since the imputer also adds indicators for imputation, \n",
    "# update the column names to include indicator columns\n",
    "imputed_cols = cols_to_impute_mean + [col + '_imputed' for col in cols_to_impute_mean]\n",
    "\n",
    "# Update the dataframe with the imputed data\n",
    "all_data[imputed_cols] = imputed_data\n",
    "\n",
    "# Optionally, you may want to update the original columns with the imputed values\n",
    "# and keep the indicators as separate columns\n",
    "for i, col in enumerate(cols_to_impute_mean):\n",
    "    all_data[col] = imputed_data[:, i]\n",
    "    all_data[col + '_imputed'] = imputed_data[:, i + len(cols_to_impute_mean)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Campagne', 'Region', 'Site', 'Famille', 'Variete', 'Num Parcelle',\n",
       "       'CodeTracabilite', 'Date', 'Tonnage', 'is_train', 'Recolte',\n",
       "       'irrigation_cumulee_1', 'irrigation_cumulee_2', 'irrigation_cumulee_3',\n",
       "       'irrigation_cumulee_4', 'irrigation_cumulee_5', 'irrigation_cumulee_6',\n",
       "       'irrigation_cumulee_7', 'irrigation_cumulee_8', 'irrigation_cumulee_9',\n",
       "       'irrigation_cumulee_10', 'irrigation_cumulee_11',\n",
       "       'irrigation_cumulee_12', 'N_cumulee_1', 'P_cumulee_1', 'K_cumulee_1',\n",
       "       'N_cumulee_2', 'P_cumulee_2', 'K_cumulee_2', 'N_cumulee_3',\n",
       "       'P_cumulee_3', 'K_cumulee_3', 'N_cumulee_4', 'P_cumulee_4',\n",
       "       'K_cumulee_4', 'N_cumulee_5', 'P_cumulee_5', 'K_cumulee_5',\n",
       "       'N_cumulee_6', 'P_cumulee_6', 'K_cumulee_6', 'N_cumulee_7',\n",
       "       'P_cumulee_7', 'K_cumulee_7', 'N_cumulee_8', 'P_cumulee_8',\n",
       "       'K_cumulee_8', 'N_cumulee_9', 'P_cumulee_9', 'K_cumulee_9',\n",
       "       'N_cumulee_10', 'P_cumulee_10', 'K_cumulee_10', 'N_cumulee_11',\n",
       "       'P_cumulee_11', 'K_cumulee_11', 'N_cumulee_12', 'P_cumulee_12',\n",
       "       'K_cumulee_12', 'Id', 'Date Plantation', 'Date Arrachage',\n",
       "       'Porte Greffe', 'Sup Debut Camp', 'Sup Plantee', 'Sup Arrachee',\n",
       "       'Sup Fin Camp', 'Tree Age', 'Days to Arrachage', 'Year', 'Month', 'Day',\n",
       "       'day', 'month', 'year', 'Days to Arrachage_imputed', 'Tree Age_imputed',\n",
       "       'Sup Debut Camp_imputed', 'irrigation_cumulee_1_imputed',\n",
       "       'irrigation_cumulee_2_imputed', 'irrigation_cumulee_3_imputed',\n",
       "       'irrigation_cumulee_4_imputed', 'irrigation_cumulee_5_imputed',\n",
       "       'irrigation_cumulee_6_imputed', 'irrigation_cumulee_7_imputed',\n",
       "       'irrigation_cumulee_8_imputed', 'irrigation_cumulee_9_imputed',\n",
       "       'irrigation_cumulee_10_imputed', 'irrigation_cumulee_11_imputed',\n",
       "       'irrigation_cumulee_12_imputed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Variete_AF1', 'Variete_AF2', 'Variete_AF3', 'Variete_CLA1',\n",
       "       'Variete_CLA10', 'Variete_CLA11', 'Variete_CLA12', 'Variete_CLA13',\n",
       "       'Variete_CLA14', 'Variete_CLA15',\n",
       "       ...\n",
       "       'irrigation_cumulee_3_imputed', 'irrigation_cumulee_4_imputed',\n",
       "       'irrigation_cumulee_5_imputed', 'irrigation_cumulee_6_imputed',\n",
       "       'irrigation_cumulee_7_imputed', 'irrigation_cumulee_8_imputed',\n",
       "       'irrigation_cumulee_9_imputed', 'irrigation_cumulee_10_imputed',\n",
       "       'irrigation_cumulee_11_imputed', 'irrigation_cumulee_12_imputed'],\n",
       "      dtype='object', length=153)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the categorical columns to be transformed\n",
    "# cat_cols = ['Region', 'Site', 'Variete', 'Porte Greffe']\n",
    "cat_cols = ['Variete', 'Porte Greffe']\n",
    "\n",
    "# Create a transformer for categorical features\n",
    "cat_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Create a column transformer to apply the transformation only to the specified categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # this will pass through other columns not listed in transformers\n",
    ")\n",
    "\n",
    "# Apply the transformations\n",
    "all_data_transformed = preprocessor.fit_transform(all_data)\n",
    "\n",
    "# The output will be a NumPy array. Convert it back to a dataframe if necessary\n",
    "# Get feature names after one-hot encoding\n",
    "new_cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
    "non_cat_cols = all_data.drop(columns=cat_cols).columns\n",
    "all_columns = list(new_cat_features) + list(non_cat_cols)\n",
    "\n",
    "# Create a new dataframe with transformed features\n",
    "all_data = pd.DataFrame(all_data_transformed, columns=all_columns)\n",
    "\n",
    "# Check the transformed dataframe\n",
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Sup Debut Camp', 'Tree Age', 'Days to Arrachage', 'Recolte']\n",
    "\n",
    "imputation_indicator = [col + '_imputed' for col in cols_to_impute_mean]\n",
    "\n",
    "monthly_params = ['N_cumulee', 'P_cumulee', 'K_cumulee', 'irrigation_cumulee']\n",
    "\n",
    "monthly_variables = [\n",
    "    f'{param}_{i}' for param in monthly_params for i in range(1, 13)]\n",
    "\n",
    "time_variables = ['year', 'month']\n",
    "\n",
    "\n",
    "# CHANGE HERE\n",
    "\n",
    "predictors_names = monthly_variables + num_cols + imputation_indicator + list(new_cat_features)\n",
    "target_name = ['Tonnage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N_cumulee_1', 'N_cumulee_2', 'N_cumulee_3', 'N_cumulee_4', 'N_cumulee_5', 'N_cumulee_6', 'N_cumulee_7', 'N_cumulee_8', 'N_cumulee_9', 'N_cumulee_10', 'N_cumulee_11', 'N_cumulee_12', 'P_cumulee_1', 'P_cumulee_2', 'P_cumulee_3', 'P_cumulee_4', 'P_cumulee_5', 'P_cumulee_6', 'P_cumulee_7', 'P_cumulee_8', 'P_cumulee_9', 'P_cumulee_10', 'P_cumulee_11', 'P_cumulee_12', 'K_cumulee_1', 'K_cumulee_2', 'K_cumulee_3', 'K_cumulee_4', 'K_cumulee_5', 'K_cumulee_6', 'K_cumulee_7', 'K_cumulee_8', 'K_cumulee_9', 'K_cumulee_10', 'K_cumulee_11', 'K_cumulee_12', 'irrigation_cumulee_1', 'irrigation_cumulee_2', 'irrigation_cumulee_3', 'irrigation_cumulee_4', 'irrigation_cumulee_5', 'irrigation_cumulee_6', 'irrigation_cumulee_7', 'irrigation_cumulee_8', 'irrigation_cumulee_9', 'irrigation_cumulee_10', 'irrigation_cumulee_11', 'irrigation_cumulee_12', 'Sup Debut Camp', 'Tree Age', 'Days to Arrachage', 'Recolte', 'Days to Arrachage_imputed', 'Tree Age_imputed', 'Sup Debut Camp_imputed', 'irrigation_cumulee_1_imputed', 'irrigation_cumulee_2_imputed', 'irrigation_cumulee_3_imputed', 'irrigation_cumulee_4_imputed', 'irrigation_cumulee_5_imputed', 'irrigation_cumulee_6_imputed', 'irrigation_cumulee_7_imputed', 'irrigation_cumulee_8_imputed', 'irrigation_cumulee_9_imputed', 'irrigation_cumulee_10_imputed', 'irrigation_cumulee_11_imputed', 'irrigation_cumulee_12_imputed', 'Variete_AF1', 'Variete_AF2', 'Variete_AF3', 'Variete_CLA1', 'Variete_CLA10', 'Variete_CLA11', 'Variete_CLA12', 'Variete_CLA13', 'Variete_CLA14', 'Variete_CLA15', 'Variete_CLA16', 'Variete_CLA17', 'Variete_CLA18', 'Variete_CLA2', 'Variete_CLA3', 'Variete_CLA4', 'Variete_CLA5', 'Variete_CLA6', 'Variete_CLA7', 'Variete_CLA8', 'Variete_CLA9', 'Variete_MAD1', 'Variete_MAD2', 'Variete_MAD3', 'Variete_MAD4', 'Variete_NOA1', 'Variete_ORA1', 'Variete_ORA10', 'Variete_ORA11', 'Variete_ORA12', 'Variete_ORA13', 'Variete_ORA14', 'Variete_ORA2', 'Variete_ORA3', 'Variete_ORA4', 'Variete_ORA5', 'Variete_ORA6', 'Variete_ORA7', 'Variete_ORA8', 'Variete_ORA9', 'Variete_SPA1', 'Variete_SPA10', 'Variete_SPA11', 'Variete_SPA2', 'Variete_SPA3', 'Variete_SPA4', 'Variete_SPA5', 'Variete_SPA6', 'Variete_SPA7', 'Variete_SPA8', 'Variete_SPA9', 'Porte Greffe_PG1', 'Porte Greffe_PG10', 'Porte Greffe_PG11', 'Porte Greffe_PG12', 'Porte Greffe_PG13', 'Porte Greffe_PG2', 'Porte Greffe_PG3', 'Porte Greffe_PG4', 'Porte Greffe_PG5', 'Porte Greffe_PG6', 'Porte Greffe_PG7', 'Porte Greffe_PG8', 'Porte Greffe_PG9', 'Porte Greffe_nan']\n"
     ]
    }
   ],
   "source": [
    "print(predictors_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_zero_na_sums = all_data[predictors_names].isna().sum()\n",
    "# non_zero_na_sums = non_zero_na_sums[non_zero_na_sums != 0]\n",
    "# non_zero_na_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[all_data['is_train'] == True]\n",
    "test = all_data[all_data['is_train'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=1, random_state=42)  # random_state for reproducibility\n",
    "X, y = train[predictors_names].values, train[target_name].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparams tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "324 fits failed out of a total of 972.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "146 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "178 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan -37586.5841715\n",
      " -37609.20999528 -37587.92763373 -37024.94929454 -36792.50437475\n",
      " -36714.85393418 -36533.60608533 -36519.67195464 -36456.19360176\n",
      " -36566.11186462 -36605.77354337 -36497.87750911 -36376.30141078\n",
      " -36395.71460874 -36414.44686649 -36261.97428058 -36383.18057678\n",
      " -36397.44033882 -36400.83467883 -36220.02480775 -36236.74035028\n",
      " -36400.83467883 -36220.02480775 -36236.74035028 -36403.08376627\n",
      " -36390.1284325  -36306.3174053  -38203.04704742 -37933.22972422\n",
      " -37759.31332595 -36766.046832   -36680.40375722 -36678.33894929\n",
      " -36468.63111249 -36463.21042037 -36422.48356492 -36734.62047444\n",
      " -36557.98709451 -36430.12466276 -36485.76091859 -36322.46663598\n",
      " -36318.78557616 -36412.84223011 -36375.04617509 -36384.10465804\n",
      " -36477.84378281 -36439.57544703 -36432.4787767  -36477.84378281\n",
      " -36439.57544703 -36432.4787767  -36629.34055601 -36531.19670998\n",
      " -36525.99324718             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -37272.11580844 -37317.20143137 -37357.4974414  -36902.048835\n",
      " -36881.34792296 -36829.54343996 -36565.2097765  -36527.43295794\n",
      " -36567.17661149 -36440.01623055 -36496.57278306 -36483.62539421\n",
      " -36600.55446552 -36596.41558071 -36575.99696335 -36385.93946849\n",
      " -36504.02876037 -36528.0667898  -36394.52458872 -36390.37240549\n",
      " -36439.54621927 -36394.52458872 -36390.37240549 -36439.54621927\n",
      " -36290.5474455  -36366.23782421 -36438.89398526 -37349.78329447\n",
      " -37052.58051239 -36977.23498904 -36580.729136   -36592.06226519\n",
      " -36611.56730332 -36738.99011133 -36633.1861981  -36579.33395519\n",
      " -36641.94238268 -36502.73118342 -36476.62055239 -36530.32461515\n",
      " -36480.99272788 -36502.89775672 -36537.76521657 -36593.0755769\n",
      " -36573.25515756 -36725.27041465 -36656.68538903 -36637.94830288\n",
      " -36725.27041465 -36656.68538903 -36637.94830288 -36587.03049378\n",
      " -36533.00306272 -36507.55921507             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan -37420.74049989 -37365.52314455 -37500.42399235\n",
      " -37207.22731122 -36959.8171804  -36779.64138847 -36385.4563312\n",
      " -36441.04286585 -36439.57943764 -36293.85229336 -36528.57588633\n",
      " -36471.83172705 -36483.1445673  -36494.43399077 -36421.21056594\n",
      " -36091.11142631 -36175.91686093 -36253.10917151 -36512.84480826\n",
      " -36350.94526188 -36334.63122554 -36512.84480826 -36350.94526188\n",
      " -36334.63122554 -36399.74866418 -36311.95086643 -36270.65254752\n",
      " -37618.57735474 -37592.91574655 -37476.48810617 -36794.74186484\n",
      " -36685.54493083 -36648.227359   -36578.58304684 -36493.06285337\n",
      " -36396.29950524 -36540.15995433 -36471.04016116 -36359.29847926\n",
      " -36611.88156163 -36378.80126065 -36406.30214813 -36485.20972802\n",
      " -36424.18703088 -36437.02083105 -36417.12784869 -36479.55445967\n",
      " -36463.88821598 -36417.12784869 -36479.55445967 -36463.88821598\n",
      " -36525.03120046 -36475.31740866 -36458.940391               nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan -37890.08242075 -37792.35100328\n",
      " -37653.76105169 -36987.3678284  -36807.21463054 -36727.12331603\n",
      " -36574.01101003 -36525.27698061 -36483.48824366 -36510.040253\n",
      " -36568.52860413 -36490.25367556 -36418.83460956 -36412.02312523\n",
      " -36399.67830437 -36254.0969461  -36382.16693995 -36416.84252217\n",
      " -36415.2043892  -36223.07530205 -36232.10246796 -36415.2043892\n",
      " -36223.07530205 -36232.10246796 -36380.66808413 -36386.37818537\n",
      " -36307.73865398 -37957.81207796 -37698.60599991 -37677.75912068\n",
      " -36528.75495614 -36499.58552894 -36545.8136847  -36344.40357864\n",
      " -36381.98415473 -36394.38774646 -36674.57624697 -36488.63137016\n",
      " -36433.30541176 -36293.99603061 -36316.05809277 -36314.72746283\n",
      " -36316.68362572 -36308.33377945 -36366.38043671 -36478.35266747\n",
      " -36431.0711933  -36428.16539654 -36478.35266747 -36431.0711933\n",
      " -36428.16539654 -36620.52784811 -36521.92063657 -36519.6263986 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Best Hyperparameters:  {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Best RMSE Score:  -36091.1114263124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a RandomForest Regressor\n",
    "selected_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=selected_model, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions and evaluation (optional)\n",
    "# predictions = best_rf.predict(X_test)\n",
    "# Print the best hyperparameters and corresponding R² score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best RMSE Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.05, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
    "# Create Extra Trees model using the best parameters\n",
    "best_model = LGBMRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                             max_depth=best_params['max_depth'],\n",
    "                                             max_features=best_params['max_features'],\n",
    "                                             min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                             min_samples_split=best_params['min_samples_split'],\n",
    "                                             learning_rate=best_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 2341, number of used features: 126\n",
      "[LightGBM] [Info] Start training from score 20375.566419\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12635\n",
      "[LightGBM] [Info] Number of data points in the train set: 2341, number of used features: 126\n",
      "[LightGBM] [Info] Start training from score 21465.653424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12622\n",
      "[LightGBM] [Info] Number of data points in the train set: 2342, number of used features: 126\n",
      "[LightGBM] [Info] Start training from score 20376.407872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12687\n",
      "[LightGBM] [Info] Number of data points in the train set: 2342, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 20929.019016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "kf = KFold(n_splits=k)\n",
    "final_results = {}\n",
    "\n",
    "train_rmse_scores = []\n",
    "train_r2_scores = []\n",
    "test_rmse_scores = []\n",
    "test_r2_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train_fold, x_test_fold = X[train_index], X[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    best_model.fit(x_train_fold, y_train_fold)\n",
    "    y_pred_train = best_model.predict(x_train_fold)\n",
    "    y_pred_test = best_model.predict(x_test_fold)\n",
    "\n",
    "    r2_train = r2_score(y_train_fold, y_pred_train)\n",
    "    rmse_train = mean_squared_error(\n",
    "        y_train_fold, y_pred_train, squared=False)\n",
    "    r2_test = r2_score(y_test_fold, y_pred_test)\n",
    "    rmse_test = mean_squared_error(y_test_fold, y_pred_test, squared=False)\n",
    "\n",
    "    train_rmse_scores.append(rmse_train)\n",
    "    train_r2_scores.append(r2_train)\n",
    "    test_rmse_scores.append(rmse_test)\n",
    "    test_r2_scores.append(r2_test)\n",
    "\n",
    "avg_train_rmse = sum(train_rmse_scores) / k\n",
    "avg_train_r2 = sum(train_r2_scores) / k\n",
    "avg_test_rmse = sum(test_rmse_scores) / k\n",
    "avg_test_r2 = sum(test_r2_scores) / k\n",
    "\n",
    "\n",
    "final_results[\"metrics\"] = {\n",
    "    \"RMSE train\": avg_train_rmse,\n",
    "    \"RMSE test\": avg_test_rmse,\n",
    "    \"R² train\": avg_train_r2,\n",
    "    \"R² test\": avg_test_r2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13036\n",
      "[LightGBM] [Info] Number of data points in the train set: 3122, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 20786.633080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataScience\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(learning_rate=0.05, max_depth=20, max_features=&#x27;sqrt&#x27;,\n",
       "              min_samples_leaf=2, min_samples_split=10, n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(learning_rate=0.05, max_depth=20, max_features=&#x27;sqrt&#x27;,\n",
       "              min_samples_leaf=2, min_samples_split=10, n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(learning_rate=0.05, max_depth=20, max_features='sqrt',\n",
       "              min_samples_leaf=2, min_samples_split=10, n_estimators=50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_features\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=2. Current value: min_data_in_leaf=2\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "X_sub = test[predictors_names].values\n",
    "\n",
    "y_sub = best_model.predict(X_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(781,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_sub)  # Convert to numpy array if not already\n",
    "assert len(y_pred) == 781, \"y_pred must have exactly 781 rows\"\n",
    "\n",
    "# Create a DataFrame. Adjust column names as per the competition's requirement.\n",
    "# Usually, you will have an ID column and a prediction column.\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': range(1, 782),  # Example: creating a sequence of IDs from 1 to 781\n",
    "    'Tonnage': y_pred\n",
    "})\n",
    "\n",
    "# Export to CSV\n",
    "csv_file = \"submission.csv\"\n",
    "submission_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear reg with regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
